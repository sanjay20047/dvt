exp 6

Haar Wavelet Transform Function CODING  


import numpy as np

def haar_wavelet_transform(data):
    n = len(data)
    output = np.copy(data)
    step = 1

    while step < n:
        for i in range(0, n, 2 * step):
            for j in range(step):
                avg = (output[i + j] + output[i + j + step]) / 2
                diff = (output[i + j] - output[i + j + step]) / 2
                output[i + j] = avg
                output[i + j + step] = diff
        step *= 2
        
    return output

# Example usage
data = [4, 6, 10, 12, 14, 16, 18, 20]
transformed_data = haar_wavelet_transform(data)
print("Transformed Data:", transformed_data)



exp 8

py1

#normal graph
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

data=np.random.normal(10.0,3,50)
sns.displot(data,bins=10,kde=True,color="black")
plt.xlabel("value")
plt.ylabel("frequency")
plt.title("normal distribution")
plt.show()

py2

#bimodal graph
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

n=400
m1,s1=80,20
m2,s2=10,20
x1=np.random.normal(m1,s1,n)
x2=np.random.normal(m2,s2,n)
x=np.concatenate([x1,x2])
sns.displot(x,bins=10,kde=True,color="black")
plt.show()

py 3

#right-skewed
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

rdata=[0]*12+[1]*34+[2]*56+[3]*78+[4]*90
sns.displot(rdata,bins=5,kde=True,alpha=0.6,color="black")
plt.xlabel("value")
plt.ylabel("frequency")
plt.title("normal distribution")
plt.show()

py4

#left-skewed
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

ldata=[0]*12+[-1]*34+[-2]*56+[-3]*78+[-4]*90
sns.displot(ldata,bins=5,kde=True,alpha=0.6,color="black")
plt.xlabel("value")
plt.ylabel("frequency")
plt.title("normal distribution")
plt.show()


py5

#uniform graph
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

data=np.random.uniform(low=0,high=1,size=600)
sns.histplot(data,bins=10,kde=True,color="black")
plt.xlabel("value")
plt.ylabel("frequency")
plt.title("normal distribution")
plt.show()




ex 9

from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch
import numpy as np
import matplotlib.pyplot as plt

# Load data from Orange
data = in_data.X

plt.figure(figsize=(10, 7))
sch.dendrogram(sch.linkage(data, method='ward'))
plt.title("Dendrogram")
plt.xlabel("Data Points")
plt.ylabel("Distance")
plt.show()


n_clusters = 5
hc = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')
cluster_labels = hc.fit_predict(data)

out_data = in_data.copy()
out_data.Y = cluster_labels.reshape(-1, 1)



exp 10

minbatchkmeans

import numpy as np
from sklearn.cluster import MiniBatchKMeans

data=np.array([
             [1,2],[1,4],[1,0],
             [4,2],[4,4],[4,0]
            ])

n_clusters=2
mkb=MiniBatchKMeans(n_clusters=n_clusters,batch_size=3,random_state=0)
cluster_labels=mkb.fit_predict(data)

print("Cluster centers: ",mkb.cluster_centers_)
print("Cluster labels: ",cluster_labels)

apriori scaling

import pandas as pd
from mlxtend.frequent_patterns import apriori,association_rules

data=pd.DataFrame({
                   "bread":[1,1,0,1,1],
                   "milk":[0,1,0,1,0],
                   "butter":[1,1,1,1,1],
                   "jam":[0,1,0,1,0],
                   "cheese":[0,0,0,1,1],
                   })
min_support=0.6
frequent_itemsets=apriori(data,min_support=min_support,use_colnames=True)
num_itemsets=len(frequent_itemsets)
min_confidence=0.7
rules=association_rules(frequent_itemsets,metric="confidence",min_threshold=min_confidence,num_itemsets=num_itemsets)
print("frequent_itemsets: ",frequent_itemsets)
print("association_rules: ",rules)
